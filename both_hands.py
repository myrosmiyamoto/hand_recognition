import cv2
import mediapipe as mp
import numpy as np

# MediaPipe Hands åˆæœŸåŒ–
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=2,  # ä¸¡æ‰‹ã‚’æ¤œå‡º
    model_complexity=0,
    min_detection_confidence=0.8,
    min_tracking_confidence=0.8
)

# æŒ‡ã®å½¢çŠ¶ã‚’åˆ¤å®šã™ã‚‹é–¢æ•°
def recognize_gesture(landmarks):
    fingers = []

    # è¦ªæŒ‡ã®åˆ¤å®š
    thumb_tip = landmarks[4]
    thumb_ip = landmarks[3]
    thumb_mcp = landmarks[2]
    thumb_up = thumb_tip.y < thumb_ip.y < thumb_mcp.y  # è¦ªæŒ‡ãŒä¸Šå‘ã
    thumb_open = abs(thumb_tip.x - thumb_mcp.x) > 0.1  # è¦ªæŒ‡ãŒé–‹ã„ã¦ã„ã‚‹

    # äººå·®ã—æŒ‡ï½å°æŒ‡ã®åˆ¤å®š
    for tip_id in [8, 12, 16, 20]:
        pip_id = tip_id - 2
        mcp_id = tip_id - 3
        is_extended = landmarks[tip_id].y < landmarks[pip_id].y < landmarks[mcp_id].y
        fingers.append(is_extended)

    # ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼èªè­˜
    if all(fingers):
        return "Paper"  # ã™ã¹ã¦ã®æŒ‡ãŒé–‹ã„ã¦ã„ã‚‹
    elif fingers[0] and fingers[1] and not fingers[2] and not fingers[3]:
        return "Scissors"  # äººå·®ã—æŒ‡ã¨ä¸­æŒ‡ãŒé–‹ã„ã¦ã„ã‚‹
    # [TODO] ğŸ‘ï¸ã®å‡¦ç†ã®åˆ¤å®šç²¾åº¦ã‚’ä¸Šã’ã‚‹ã®ãŒå›°é›£
    elif thumb_up and not any(fingers):
        return "Good"  # è¦ªæŒ‡ãŒç«‹ã£ã¦ã„ã¦ã€ä»–ã®æŒ‡ãŒé–‰ã˜ã¦ã„ã‚‹
    elif not any(fingers):
        return "Rock"  # ã™ã¹ã¦ã®æŒ‡ãŒé–‰ã˜ã¦ã„ã‚‹

    return "Unknown Gesture"

# ã‚«ãƒ¡ãƒ©èµ·å‹•
cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
# ç”»åƒã®å¹…ã¨é«˜ã•ã‚’è¨­å®š
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        print("ã‚«ãƒ¡ãƒ©ãŒæ¤œå‡ºã§ãã¾ã›ã‚“ã€‚")
        break
    frame = cv2.resize(frame, (width, height), interpolation=cv2.INTER_LINEAR)

    # ç”»åƒã‚’åè»¢ã—ã¦å·¦å³ã‚’èª¿æ•´
    frame = cv2.flip(frame, 1)

    # ç”»åƒã‚’RGBã«å¤‰æ›
    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    frame.flags.writeable = False

    # æ‰‹ã®ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã‚’æ¤œå‡º
    results = hands.process(frame)
    frame.flags.writeable = True
    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)

    left_hand_gesture = None
    right_hand_gesture = None

    if results.multi_hand_landmarks:
        for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):
            # æ‰‹ã®å‘ãã‚’å–å¾—
            handedness = results.multi_handedness[idx].classification[0].label  # "Left" ã¾ãŸã¯ "Right"

            # ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã‚’æç”»
            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

            # æŒ‡ã®å½¢çŠ¶ã‚’èªè­˜
            gesture = recognize_gesture(hand_landmarks.landmark)

            # å³æ‰‹ãƒ»å·¦æ‰‹ã®åˆ¤å®š
            if handedness == "Left":
                left_hand_gesture = gesture
            else:
                right_hand_gesture = gesture

            # ç‰‡æ‰‹ã®çµæœã‚’è¡¨ç¤º
            cv2.putText(frame, f"{handedness}: {gesture}", (10, 50 + idx * 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

    # ä¸¡æ‰‹ãŒã€Œãƒ‘ãƒ¼ï¼ˆPaperï¼‰ã€ã®å ´åˆ
    if left_hand_gesture == "Paper" and right_hand_gesture == "Paper":
        cv2.putText(frame, "Both Hands Paper", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 3, cv2.LINE_AA)

    # æ˜ åƒã‚’è¡¨ç¤º
    cv2.imshow('Hand Gesture Recognition', frame)

    # 'q'ã‚­ãƒ¼ã§çµ‚äº†
    if cv2.waitKey(10) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
